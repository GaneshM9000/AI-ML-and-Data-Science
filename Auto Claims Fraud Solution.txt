--------------------------------------------------------------------------------------------------------------------------------------
Auto Claims Fraud Solution
--------------------------------------------------------------------------------------------------------------------------------------
# Importing the libraries 

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns

# Ignore harmless warnings 

import warnings 
warnings.filterwarnings("ignore")

# Set to display all the columns in dataset

pd.set_option("display.max_columns", None)
--------------------------------------------------------------------------------------------------------------------------------------
# load the Auto Insurance dataset 

Claims_Fraud = pd.read_excel(r"E:\R3SPAnalytics\00 IIT KGP Hyd\ILoveHR\Data-02\Auto_Insurance_Claims_Fraud_V1.0.xlsx", 
                             sheet_name="AutoInsClaims", header=0)

# Copy to back-up file

Claims_Fraud_BK = Claims_Fraud.copy()

# Display the first 5 records

Claims_Fraud.head()
--------------------------------------------------------------------------------------------------------------------------------------
# Checking the dataset for balanced or imbalanced?
# Count the target or dependent variable by '0' & '1' and 
# their proportion (> 10 : 1, then the dataset is imbalance dataset)
                    
Target_count = Claims_Fraud.Fraud_Investigation.value_counts()
print('Class 0:', Target_count[0])
print('Class 1:', Target_count[1])
print('Proportion:', round(Target_count[0] / Target_count[1], 2), ': 1')
print('Total loans Trans:', len(Claims_Fraud))
--------------------------------------------------------------------------------------------------------------------------------------
# Shape of the dataset

Claims_Fraud.shape
--------------------------------------------------------------------------------------------------------------------------------------
# Display the information of the dataset

Claims_Fraud.info()
--------------------------------------------------------------------------------------------------------------------------------------
# 'map' function to covert the categorical values to discret numerical format

Claims_Fraud['Gender'] = Claims_Fraud['Gender'].map({'F':0,'M':1})
--------------------------------------------------------------------------------------------------------------------------------------
Claims_Fraud = Claims_Fraud.drop(['Household_Num', 'Driver_Num', 'Policy_Num', 'Claim_Num', 'Accident_Date', 'Claim_Initiated_Date',
                                  'Primary_Driver_Num', 'Policy_Start_Date', 'Policy_Expiry_Date', 'Model_Year', 'Make', 'Model', 
                                  'License_Plate', 'Color', 'Initial_Odometer', 'Low_Milege_Usage', 'Drivers_License_Num', 'DL_Expiry_Date',
                                  'Drivers_License_State', 'Date_At_Current_Address', 'SC_Time', 'DLE_Days', 'S_Living', 'No_of_Miles',
                                  'Plcy_Year', 'Miles_Year', 'Claim_Apply_Diff_Days', 'Odometer_At_Incident'], axis=1)

Claims_Fraud.head(4)
--------------------------------------------------------------------------------------------------------------------------------------
# Display the information of the dataset

Claims_Fraud.info()
--------------------------------------------------------------------------------------------------------------------------------------
# Prepare the list for scaling

Scaling_Var = ['Incident_Cause', 'Claims_Status', 'Claim_Amount', 'Age']
--------------------------------------------------------------------------------------------------------------------------------------
# Identify the independent and Target variables

IndepVar = []
for col in Claims_Fraud.columns:
    if col != 'Fraud_Investigation':
        IndepVar.append(col)

TargetVar = 'Fraud_Investigation'

x = Claims_Fraud[IndepVar]
y = Claims_Fraud[TargetVar]
--------------------------------------------------------------------------------------------------------------------------------------
# Split the data into train and test

from sklearn.model_selection import train_test_split 

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=143)

# Display the shape of the train_data and test_data

x_train.shape, y_train.shape, x_test.shape, y_test.shape
--------------------------------------------------------------------------------------------------------------------------------------
# Scaling the features by using MinMaxScaler

from sklearn.preprocessing import MinMaxScaler

mmscaler = MinMaxScaler(feature_range=(0, 1))

x_train[Scaling_Var] = mmscaler.fit_transform(x_train[Scaling_Var])
x_train = pd.DataFrame(x_train)

x_test[Scaling_Var] = mmscaler.fit_transform(x_test[Scaling_Var])
x_test = pd.DataFrame(x_test)
--------------------------------------------------------------------------------------------------------------------------------------
# Load the results dataset

EMResults = pd.read_csv(r"E:\R3SPAnalytics\00 IIT KGP Hyd\ILoveHR\Data-02\EMResults.csv", header=0)

# Display the first 5 records

EMResults.head()
--------------------------------------------------------------------------------------------------------------------------------------
#Compare the Classification Algorithms
--------------------------------------------------------------------------------------------------------------------------------------
# Build the Calssification models and compare the results

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Create objects of classification algorithm with default hyper-parameters

ModelLR = LogisticRegression()
ModelDC = DecisionTreeClassifier()
ModelRF = RandomForestClassifier()
ModelET = ExtraTreesClassifier()
ModelKNN = KNeighborsClassifier(n_neighbors=5)
ModelGNB = GaussianNB()
ModelSVM = SVC(probability=True)

# Evalution matrix for all the algorithms

MM = [ModelLR, ModelDC, ModelRF, ModelET, ModelKNN, ModelGNB, ModelSVM]
for models in MM:
    
    # Fit the model
    
    models.fit(x_train, y_train)
    
    # Prediction
    
    y_pred = models.predict(x_test)
    y_pred_prob = models.predict_proba(x_test)
    
    # Print the model name
    
    print('Model Name: ', models)
    
    # confusion matrix in sklearn

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import classification_report

    # actual values

    actual = y_test

    # predicted values

    predicted = y_pred

    # confusion matrix

    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)
    print('Confusion matrix : \n', matrix)

    # outcome values order in sklearn

    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)
    print('Outcome values : \n', tp, fn, fp, tn)

    # classification report for precision, recall f1-score and accuracy

    C_Report = classification_report(actual,predicted,labels=[1,0])

    print('Classification report : \n', C_Report)

    # calculating the metrics

    sensitivity = round(tp/(tp+fn), 3);
    specificity = round(tn/(tn+fp), 3);
    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);
    balanced_accuracy = round((sensitivity+specificity)/2, 3);
    
    precision = round(tp/(tp+fp), 3);
    f1Score = round((2*tp/(2*tp + fp + fn)), 3);

    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. 
    # A model with a score of +1 is a perfect model and -1 is a poor model

    from math import sqrt

    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)
    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)

    print('Accuracy :', round(accuracy*100, 2),'%')
    print('Precision :', round(precision*100, 2),'%')
    print('Recall :', round(sensitivity*100,2), '%')
    print('F1 Score :', f1Score)
    print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )
    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')
    print('MCC :', MCC)

    # Area under ROC curve 

    from sklearn.metrics import roc_curve, roc_auc_score

    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))
    
    # ROC Curve
    
    from sklearn.metrics import roc_auc_score
    from sklearn.metrics import roc_curve
    model_roc_auc = roc_auc_score(actual, predicted)
    fpr, tpr, thresholds = roc_curve(actual, models.predict_proba(x_test)[:,1])
    plt.figure()
    # plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
    plt.plot(fpr, tpr, label= 'Classification Model' % model_roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig('Log_ROC')
    plt.show()
    print('-----------------------------------------------------------------------------------------------------')
    #---
    new_row = {'Model Name' : models,
               'True_Positive' : tp, 
               'False_Negative' : fn, 
               'False_Positive' : fp,
               'True_Negative' : tn,
               'Accuracy' : accuracy,
               'Precision' : precision,
               'Recall' : sensitivity,
               'F1 Score' : f1Score,
               'Specificity' : specificity,
               'MCC':MCC,
               'ROC_AUC_Score':roc_auc_score(actual, predicted),
               'Balanced Accuracy':balanced_accuracy}
    EMResults = EMResults._append(new_row, ignore_index=True)
    #-----------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------
# Results with comparing the all the algorithms 

#EMResults.to_csv("E://R3SPAnalytics//00 IIT KGP Hyd//ILoveHR//Data-02//EMResults04.csv")

EMResults.head(10)
--------------------------------------------------------------------------------------------------------------------------------------
# Sorting by multiple columns: first by Age (ascending), then by Salary (descending)

EMResults_New = EMResults.sort_values(by=["Accuracy", "F1 Score", "ROC_AUC_Score", "True_Positive", 
                                          "False_Positive", "False_Negative", "True_Negative"],
                                      ascending=[False, False, False, False, True, True, False])

# Display the Algorithms Compare result

#EMResults_New = pd.DataFrame(EMResults_New)
EMResults_New.head(10)
--------------------------------------------------------------------------------------------------------------------------------------

