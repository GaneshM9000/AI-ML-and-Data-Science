--------------------------------------------------------------------------------------------------------------------------------------
# Loan Defaulter Predictions
--------------------------------------------------------------------------------------------------------------------------------------
# Importing the libraries 

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
from matplotlib import pyplot

# Ignore harmless warnings 

import warnings 
warnings.filterwarnings("ignore")

# Set to display all the columns in dataset

pd.set_option("display.max_columns", None)
--------------------------------------------------------------------------------------------------------------------------------------
# load the Auto Insurance Claims data 

LDP_Data = pd.read_csv(r"E:\R3SPAnalytics\00 IIT KGP Hyd\ILoveHR\Data-01\Loan_Default_Prediction.csv", header=0)

# Copy file t back-up file

LDP_Data_bk = LDP_Data.copy()

# display first five records

LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
# Display the dataset information

LDP_Data.info()
--------------------------------------------------------------------------------------------------------------------------------------
# Checking the dataset for balanced or imbalanced?
# Count the target or dependent variable by '0' & '1' and 
# their proportion (> 10 : 1, then the dataset is imbalance dataset)
                    
Target_count = LDP_Data.risk_of_default.value_counts()
print('Class 0:', Target_count[0])
print('Class 1:', Target_count[1])
print('Proportion:', round(Target_count[0] / Target_count[1], 2), ': 1')
print('Total Loans Trans:', len(LDP_Data))
--------------------------------------------------------------------------------------------------------------------------------------
# Check the null values in the variables
LDP_Data.isnull().sum()
--------------------------------------------------------------------------------------------------------------------------------------
# Feature engineering 01 - Calculation of "Income to Loan Ratio"
LDP_Data["income_to_loan_ratio"] = LDP_Data["annual_income"] / LDP_Data["loan_amount"]

# Feature engineering 02 - Calculation of "Monthly Installment"
LDP_Data["monthly_installment"] = LDP_Data["loan_amount"] * (1 + (LDP_Data["interest_rate"] / 100)) / LDP_Data["loan_term_months"]

# Feature engineering 03 - Calculation of "Monthly Installment"
LDP_Data["loan_to_property_value_ratio"] = LDP_Data["loan_amount"] / LDP_Data["property_value"]

# Feature engineering 04 - Calculation of "Monthly Installment"
LDP_Data["installment_to_income_ratio"] = LDP_Data["monthly_installment"] / (LDP_Data["annual_income"] / 12)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
# Feature engineering 05 - Calculation of "Financial Burden Score"
LDP_Data["financial_burden_score"] = LDP_Data["debt_to_income_ratio"] + (LDP_Data["monthly_installment"] / (LDP_Data["annual_income"] / 12)) * 100

# Feature engineering 06 - Calculation of "Dependency Ratio"
LDP_Data["dependency_ratio"] = LDP_Data["num_dependents"] / (LDP_Data["borrower_age"] - 22 + 1)

# Feature engineering 07 - Calculation of "Financial Stability Score"
LDP_Data["financial_stability_score"] = LDP_Data["income_stability"] * 100 - LDP_Data["unemployment_risk_factor"] * 100

# Feature engineering 08 - Calculation of "Net Assets"
LDP_Data["net_assets"] = LDP_Data["assets_value"] - (LDP_Data["credit_card_debt"] + LDP_Data["personal_loan_debt"] + LDP_Data["mortgage_debt"])

# Feature engineering 09 - Calculation of "Total Debt"
LDP_Data["total_debt"] = LDP_Data["credit_card_debt"] + LDP_Data["personal_loan_debt"] + LDP_Data["mortgage_debt"]

# Feature engineering 10 - Calculation of "Debt to Assets Ratio"
LDP_Data["debt_to_assets_ratio"] = LDP_Data["total_debt"] / LDP_Data["assets_value"]

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
# Create a Age range for the "Borrower Age"
LDP_Data["age_bucket"] = pd.cut(LDP_Data["borrower_age"], bins=[22, 30, 45, 60, 65], labels=["22-30", "30-45", "45-60", "60-65"])

# Create industry standard label for the "Credit Score"
LDP_Data["credit_risk_level"] = pd.cut(LDP_Data["credit_score"], bins=[300, 579, 669, 739, 799, 850],
                                       labels=["Poor", "Fair", "Good", "Very Good", "Excellent"])

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
# Create Loan Default Lending Indicator (Probability)
--------------------------------------------------------------------------------------------------------------------------------------
# Create "Recent Default Indicator" 
LDP_Data.loc[LDP_Data["recent_default"] >= 1, 'Recent_Default_Ind'] = 1
LDP_Data.loc[LDP_Data["recent_default"] < 1, 'Recent_Default_Ind'] = 0
LDP_Data["Recent_Default_Ind"] = LDP_Data["Recent_Default_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['Recent_Default_Ind'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
# Create "Credit Score Indicator" 
LDP_Data.loc[LDP_Data["credit_score"] <= 600, 'Credit_Score_Ind'] = 1
LDP_Data.loc[LDP_Data["credit_score"] > 600, 'Credit_Score_Ind'] = 0
LDP_Data["Credit_Score_Ind"] = LDP_Data["Credit_Score_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['Credit_Score_Ind'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
# Create "Missed Payment Indicator" 
LDP_Data.loc[LDP_Data["missed_payments"] > 2, 'Missed_Payments_Ind'] = 1
LDP_Data.loc[LDP_Data["missed_payments"] <= 2, 'Missed_Payments_Ind'] = 0
LDP_Data["Missed_Payments_Ind"] = LDP_Data["Missed_Payments_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['Missed_Payments_Ind'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
# Create "Dependency Ratio Indicator" 
LDP_Data.loc[LDP_Data["dependency_ratio"] > 0.5 , 'Dependency_Ratio_Ind'] = 1
LDP_Data.loc[LDP_Data["dependency_ratio"] <= 0.5, 'Dependency_Ratio_Ind'] = 0
LDP_Data["Dependency_Ratio_Ind"] = LDP_Data["Dependency_Ratio_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['Dependency_Ratio_Ind'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
# Create "Debt to Income Ratio Indicator" 
LDP_Data.loc[LDP_Data["debt_to_income_ratio"] > 35 , 'Debt_to_Income_Ratio_Ind'] = 1
LDP_Data.loc[LDP_Data["debt_to_income_ratio"] <= 35, 'Debt_to_Income_Ratio_Ind'] = 0
LDP_Data["Debt_to_Income_Ratio_Ind"] = LDP_Data["Debt_to_Income_Ratio_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['Debt_to_Income_Ratio_Ind'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
# Create "Loan to Property Value Ratio Indicator" 
LDP_Data.loc[LDP_Data["loan_to_property_value_ratio"] > 0.8 , 'Loan_to_Property_Value_Ratio_Ind'] = 1
LDP_Data.loc[LDP_Data["loan_to_property_value_ratio"] <= 0.8, 'Loan_to_Property_Value_Ratio_Ind'] = 0
LDP_Data["Loan_to_Property_Value_Ratio_Ind"] = LDP_Data["Loan_to_Property_Value_Ratio_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['Loan_to_Property_Value_Ratio_Ind'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
# Create "Income Stability Indicator" 
LDP_Data.loc[LDP_Data["income_stability"] > 0.75 , 'Income_Stability_Ind'] = 1
LDP_Data.loc[LDP_Data["income_stability"] <= 0.75, 'Income_Stability_Ind'] = 0
LDP_Data["Income_Stability_Ind"] = LDP_Data["Income_Stability_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['Income_Stability_Ind'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
# Create "Unemployment Risk Factor Indicator" 
LDP_Data.loc[LDP_Data["unemployment_risk_factor"] > 0.75 , 'Unemployment_Risk_Factor_Ind'] = 1
LDP_Data.loc[LDP_Data["unemployment_risk_factor"] <= 0.75, 'Unemployment_Risk_Factor_Ind'] = 0
LDP_Data["Unemployment_Risk_Factor_Ind"] = LDP_Data["Unemployment_Risk_Factor_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['Unemployment_Risk_Factor_Ind'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data['defaulted_before'].value_counts()
--------------------------------------------------------------------------------------------------------------------------------------
# Display the column name of the LDP data
LDP_Data.columns
--------------------------------------------------------------------------------------------------------------------------------------
# Label Encoding the varibales
--------------------------------------------------------------------------------------------------------------------------------------
# 'map' function to covert the categorical values to discret numerical format

LDP_Data['employment_status'] = LDP_Data['employment_status'].map({'Employed':0,'Self-employed':1, 'Retired':2, 'Unemployed':3})
LDP_Data['industry_employment'] = LDP_Data['industry_employment'].map({'Retail':0,'Tech':1, 'Healthcare':2, 'Construction':3,
                                                                      'Education':4, 'Finance':5})
LDP_Data['vehicle_ownership'] = LDP_Data['vehicle_ownership'].map({'Owned':0,'Leased':1})
LDP_Data['education_level'] = LDP_Data['education_level'].map({'PhD':0,'Masters':1, 'Bachelors':2, 'High School':3})
LDP_Data['marital_status'] = LDP_Data['marital_status'].map({'Married':0,'Widowed':1, 'Divorced':2, 'Single':3})
LDP_Data['loan_purpose'] = LDP_Data['loan_purpose'].map({'Debt Consolidation':0,'Vehicle Purchase':1, 'Education':2, 
                                                         'Business':3, 'Medical':4, 'Home Improvement':5})
LDP_Data['home_ownership'] = LDP_Data['home_ownership'].map({'Owned':0,'Mortgaged':1, 'Rented':2})
--------------------------------------------------------------------------------------------------------------------------------------
LDP_Data = LDP_Data.drop(['loan_id', 'customer_id', 'credit_score', 'num_dependents', 'property_value', 'monthly_debt_payments',
                         'debt_to_income_ratio', 'missed_payments', 'recent_default', 'credit_card_debt', 'personal_loan_debt',
                         'assets_value', 'credit_risk_level', 'income_to_loan_ratio', 'monthly_installment',
                          'loan_to_property_value_ratio', 'installment_to_income_ratio', 'financial_burden_score', 'dependency_ratio', 
                          'financial_stability_score', 'net_assets', 'total_debt', 'debt_to_assets_ratio', 'age_bucket'], axis=1)
--------------------------------------------------------------------------------------------------------------------------------------
Scaling_Var = ['borrower_age', 'annual_income', 'loan_amount', 'loan_term_months', 'interest_rate', 'education_level',
              'marital_status', 'loan_purpose', 'home_ownership', 'employment_length_years', 'residence_stability_years',
              'years_since_first_credit', 'max_credit_limit', 'avg_monthly_spending', 'credit_usage_percentage',
              'payment_history_percentage', 'loan_history_years', 'loan_to_asset_ratio', 'industry_employment',
              'income_stability', 'vehicle_ownership', 'unemployment_risk_factor']
--------------------------------------------------------------------------------------------------------------------------------------
#Re-cheking of missing values in LDP data
LDP_Data.isnull().sum()
--------------------------------------------------------------------------------------------------------------------------------------
# Identify the Independent and Target variables

IndepVar = []
for col in LDP_Data.columns:
    if col != 'risk_of_default':
        IndepVar.append(col)

TargetVar = 'risk_of_default'

x = LDP_Data[IndepVar]
y = LDP_Data[TargetVar]
--------------------------------------------------------------------------------------------------------------------------------------
# Random oversampling can be implemented using the RandomOverSampler function

from imblearn.over_sampling import RandomOverSampler

oversample = RandomOverSampler(sampling_strategy=0.105)

x_over, y_over = oversample.fit_resample(x, y)

print(x_over.shape)
print(y_over.shape)
--------------------------------------------------------------------------------------------------------------------------------------
# Merge two Dataframes x_over, y_over on index of both the dataframes to get file

LDP_Check = x_over.merge(y_over, left_index=True, right_index=True)

LDP_Check.shape
--------------------------------------------------------------------------------------------------------------------------------------
# Count the target or dependent variable by '0', '1' & their proportion (> 10 : 1, then the dataset is imbalance dataset)
# After over sample technique applied

Target_count = LDP_Check.risk_of_default.value_counts()
print('Class 0:', Target_count[0])
print('Class 1:', Target_count[1])
print('Proportion:', round(Target_count[0] / Target_count[1], 2), ': 1')
print('Total Loans Trans:', len(LDP_Check))
--------------------------------------------------------------------------------------------------------------------------------------
# Splitting the dataset into train and test 

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_over, y_over, test_size = 0.30, random_state = 42)

# Display the shape of train and test data 

x_train.shape, x_test.shape, y_train.shape, y_test.shape
--------------------------------------------------------------------------------------------------------------------------------------
# Scaling the features by using MinMaxScaler

from sklearn.preprocessing import MinMaxScaler

mmscaler = MinMaxScaler(feature_range=(0, 1))

x_train[Scaling_Var] = mmscaler.fit_transform(x_train[Scaling_Var])
x_train = pd.DataFrame(x_train)

x_test[Scaling_Var] = mmscaler.fit_transform(x_test[Scaling_Var])
x_test = pd.DataFrame(x_test)
--------------------------------------------------------------------------------------------------------------------------------------
# Compare Classification algorithms
--------------------------------------------------------------------------------------------------------------------------------------
# Load the results dataset

EMResults = pd.read_csv(r"E:\R3SPAnalytics\01-SDP\Datasets\Results\EMResults.csv", header=0)

# Display the first 5 records

EMResults.head()
--------------------------------------------------------------------------------------------------------------------------------------
# Build the Calssification models and compare the results

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier
#from sklearn.neighbors import KNeighborsClassifier
#from sklearn.naive_bayes import GaussianNB
#from sklearn.svm import SVC
#from xgboost import XGBClassifier
# import lightgbm as lgb

# Create objects of classification algorithm with default hyper-parameters

ModelLR = LogisticRegression()
ModelDC = DecisionTreeClassifier()
ModelRF = RandomForestClassifier()
ModelET = ExtraTreesClassifier()
#ModelKNN = KNeighborsClassifier(n_neighbors=5)
#ModelGNB = GaussianNB()
#ModelSVM = SVC(probability=True)

# Evalution matrix for all the algorithms

#MM = [ModelLR, ModelDC, ModelRF, ModelET, ModelKNN, ModelGNB, ModelSVM, modelXGB, modelLGB]
MM = [ModelLR, ModelDC, ModelRF, ModelET]
for models in MM:
    
    # Fit the model
    
    models.fit(x_train, y_train)
    
    # Prediction
    
    y_pred = models.predict(x_test)
    y_pred_prob = models.predict_proba(x_test)
    
    # Print the model name
    
    print('Model Name: ', models)
    
    # confusion matrix in sklearn

    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import classification_report

    # actual values

    actual = y_test

    # predicted values

    predicted = y_pred

    # confusion matrix

    matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)
    print('Confusion matrix : \n', matrix)

    # outcome values order in sklearn

    tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)
    print('Outcome values : \n', tp, fn, fp, tn)

    # classification report for precision, recall f1-score and accuracy

    C_Report = classification_report(actual,predicted,labels=[1,0])

    print('Classification report : \n', C_Report)

    # calculating the metrics

    sensitivity = round(tp/(tp+fn), 3);
    specificity = round(tn/(tn+fp), 3);
    accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);
    balanced_accuracy = round((sensitivity+specificity)/2, 3);
    
    precision = round(tp/(tp+fp), 3);
    f1Score = round((2*tp/(2*tp + fp + fn)), 3);

    # Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. 
    # A model with a score of +1 is a perfect model and -1 is a poor model

    from math import sqrt

    mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)
    MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)

    print('Accuracy :', round(accuracy*100, 2),'%')
    print('Precision :', round(precision*100, 2),'%')
    print('Recall :', round(sensitivity*100,2), '%')
    print('F1 Score :', f1Score)
    print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )
    print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')
    print('MCC :', MCC)

    # Area under ROC curve 

    from sklearn.metrics import roc_curve, roc_auc_score

    print('roc_auc_score:', round(roc_auc_score(actual, predicted), 3))
    
    # ROC Curve
    
    from sklearn.metrics import roc_auc_score
    from sklearn.metrics import roc_curve
    model_roc_auc = roc_auc_score(actual, predicted)
    fpr, tpr, thresholds = roc_curve(actual, models.predict_proba(x_test)[:,1])
    plt.figure()
    # plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
    plt.plot(fpr, tpr, label= 'Classification Model' % model_roc_auc)
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig('Log_ROC')
    plt.show()
    print('-----------------------------------------------------------------------------------------------------')
    #---
    new_row = {'Model Name' : models,
               'True_Positive' : tp, 
               'False_Negative' : fn, 
               'False_Positive' : fp,
               'True_Negative' : tn,
               'Accuracy' : accuracy,
               'Precision' : precision,
               'Recall' : sensitivity,
               'F1 Score' : f1Score,
               'Specificity' : specificity,
               'MCC':MCC,
               'ROC_AUC_Score':roc_auc_score(actual, predicted),
               'Balanced Accuracy':balanced_accuracy}
    EMResults = EMResults._append(new_row, ignore_index=True)
    #-------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------
# Results with comparing the all the algorithms 

#EMResults.to_csv("D://R3SPAnalytics//01-SDP//Results//EMResults_02.csv")

EMResults.head(10)
--------------------------------------------------------------------------------------------------------------------------------------
# Sorting by multiple columns: first by Age (ascending), then by Salary (descending)

EMResults_New = EMResults.sort_values(by=["Accuracy", "F1 Score", "ROC_AUC_Score", "True_Positive", 
                                          "False_Positive", "False_Negative", "True_Negative"],
                                      ascending=[False, False, False, False, True, True, False])

# Display the Algorithms Compare result

#EMResults_New = pd.DataFrame(EMResults_New)
EMResults_New.head(10)
--------------------------------------------------------------------------------------------------------------------------------------




Nagaraju Gundala
19:50
# Create "Missed Payment Indicator" 
LDP_Data.loc[LDP_Data["missed_payments"] > 2, 'Missed_Payments_Ind'] = 1
LDP_Data.loc[LDP_Data["missed_payments"] <= 2, 'Missed_Payments_Ind'] = 0
LDP_Data["Missed_Payments_Ind"] = LDP_Data["Missed_Payments_Ind"].astype(int)

# Display first 5 records
LDP_Data.head()
